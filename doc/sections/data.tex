\section{The CPD Data}

The original raw data files released by the CPD, 
the code to clean and link the data,
and the code to generate this documentation is 
all available online at \url{https://github.com/chicago-police-violence/data}.

\paragraph{Technical choices.} Code is written in Python, use of \texttt{Make}
to coordinate the various processing steps and easily reproduce them.
\textbf{TODO:} say a bit more about the requirements of the environment, or
maybe simply refer to the README?



\subsection{Initial Cleaning}

The files initially released by the CPD as a reply to the FOIA requests are for
the most part Excel spreadsheets, with inconsistent formatting and which can
thus be difficult to process programmatically. As an example, the reader is
invited to open the file
\texttt{p046957\_-\_report\_1.1\_-\_all\_complaints\_in\_time\_frame.xls}
available in the folder \texttt{raw/P0-46957/}. As can be seen, each record in
this file is spread over two rows of the spreadsheet, with the field names
repeated at the beginning of the second row for each record.

The goal of the cleaning step is thus to produce ``reasonable'' CSV files from
the original files, with the minimum requirement that each record be presented
on a single line after this step. The code for this cleaning step is contained
in the files \texttt{datasets.py}, \texttt{parse.py} and
\texttt{parse\_p046957.py} in the \texttt{src/} folder and the entire step can
be applied by running \texttt{make parse} in the root of the repository. This
creates the folder \texttt{parsed/} containing the clean CSV files.

As can be seen by inspecting the code, the decisions made at this stage are, we
believe, uncontroversial as they only consists of:
\begin{itemize}
	\item unifying field names across datasets, so that the same type of data
		is always identified in the saw way (for example, \texttt{Appointment
		Date}, \texttt{Appt Date}, \texttt{appointment\_date} are all mapped to
		\texttt{appointment\_date}.
	\item unifying field values across datasets. For example, the gender of an
		officer is indicated as a single letter \texttt{M} or \texttt{F} in
		some datasets or as \texttt{Male}, \texttt{Female} (based on the data
		release, it does not seem that the system used by the CPD has an option
		to represent non-binary officers).
	\item parsing values into the correct data type or format. For example,
		dates are formatted differently depending on the dataset, and we map
		everything to the ISO 8601 format.
\end{itemize}

Consequently, for someone planning to use the data in the present repository,
there is virtually no reason not to start at the minimum from the output of
this cleaning step. The subsequent steps required making more difficult and
debatable decisions, so depending on the application, researchers might want to
perform them differently, but in all cases, those alternative decisions can
branch off from the output of the cleaning step.

\subsection{Linking and Merging Datasets}\label{sec:linking}

\subsubsection{Officer matching}

As already alluded to, the main challenge at this step is that there is no
identifier uniquely identifying officers across records. In other words, there
is no foolproof way to know if two different records correspond to the same
officer, \emph{even within the same dataset} (for example the same officer
could appear with slightly different attributes on two different complaint
records).

We thus need to design a matching method striking a balance between
\begin{itemize}
	\item being loose enough to avoid type II error (false negatives). If the
		same officer appears with slightly different attributes across two
		records, we do not want our matching method to believe it is two
		different officers.
	\item being strict enough to avoid type I error (false positives). We do
		not want to merge two different officers into a single identity.
\end{itemize}

The difficulty in achieving this balance is that perhaps surprisingly
\emph{none of the attributes of a given police officer are guaranteed to be
stable over time}. Most notably, officers' names change over time, for example
to fix data entry errors or in case of legal name changes. However, we observed
two attributes, present in almost all original datasets and which seem
remarkably stable over the time: \emph{appointment date} and \emph{birthyear}.
These two attributes thus proved very valuable to disambiguate officers with
identical names.

In order to match officers across two datasets, we developed an \emph{iterative
pairwise matching procedure} that makes iterative passes over the datasets.
During each pass, a subset of the officer attributes present in both datasets
is selected as the matching criterion, and a pair of officers (one from each
dataset) is identified as a \emph{match} if (i) their attributes from the
chosen subset match, and (ii) if they are the only two officers matching on
these attributes. Once a pair is identified as a match, it is put aside, and
the next pass is performed on the remaining unmatched officers. After all the
passes are done the leftover officers are declared as different officers. By
constructing a hash table mapping a subset of attributes to the list of
officers sharing these attributes, each pass can be performed in linear time,
so the overall running time of the procedure is $O\big(P(N_1+N_2)\big)$ where
$P$ is the number of passes and $N_1, N_2$ are the number of officers in each
dataset.

To fully specify the matching procedure we thus need to specify which subset of
attributes is chosen as the matching criterion at each pass. For this, we go
from the stricter to the looser criterion: for the first pass, we choose
a subset \emph{all} the attributes which are present in two datasets, and then
start removing attributes one by one. For example, one can remove the
\emph{last name} attribute for the second pass to match a pair of officers
whose last names are different but match on all the remaining attributes, thus
identifying an officer whose last name changed between the releases of the two
datasets. The advantage of going from stricter to looser is two fold:
\begin{itemize}
	\item starting from the strictest set of attributes identifies the
		\emph{unambiguously matching pairs}, that is, the ones which are
		a clear match and which, thankfully, constitute the vast majority of
		officers (typically around 80-90\% of officers are matched during the
		first pass \textbf{TODO check}). Since the next passes will only be
		performed on the remaining unmatched officers, this removes a lot of
		potential ambiguities which could occur once the set of attributes is
		reduced.
	\item since the vast majority of officers is matched during the first pass,
		it becomes feasible during the subsequent passes to visually inspect all
		the matched pairs and assess whether the chosen set of attributes was
		too strict or too lose (\textbf{TODO:} explain how to activate
		debugging information in the code).
\end{itemize}

We note that there is still some amount of subjective judgment involved,
following a visual inspection of the second and subsequent passes, to decide
which sets attributes are ``acceptable'' (that is, for which the probability of
two persons sharing these attributes in a population of the size of the CPD is
extremely small). This is also how we decide that sufficiently many passes have
been performed: when it would seem likely to introduce a type I error by
matching any of the remaining officers. As a general rule of thumb we erred on
the side of favoring type II errors over type I errors. That is, we only
matched officers when it would seem extremely unlikely that they correspond to
two different individuals).

\textbf{TODO:} table of officer attributes in each dataset

\textbf{TODO:} explain the few subtleties where we don't use equality to match
attributes (for example when matching age and birthyear, where the age only
lets us identify the birthyear with an accuracy of 1 year). Or with stars,
where we test whether a star number is contained in the subset of known stars
for this officer.

With this procedure at hand we can thus link officers across datasets starting
from the most similar datasets first (for which we expect to have the least
amount of ambiguity). That is we first link \texttt{P0-58155} to
\texttt{P4-41436}, then \texttt{P0-52262} to \texttt{16-1105} and then the
remaining datasets \textbf{expand}

\textbf{TODO} give in appendix table summarizing which subset of attributes are
used at each linking operation 

\subsubsection{Roster consolidation}

A byproduct of matching officers across datasets is that for each officer, we
now have as many “profiles” as the number of datasets in which they appear,
where by \emph{profile} we mean a collection of attributes. Note that each
profile can contain a different subset of attributes (since not all attributes
are present in each dataset) and that a given attribute might take a different
value in different profiles of the same officer (since the iterative matching
procedure is not restricted to performing strict matching).

We are thus faced with the task of “consolidating” the different profiles of
a given officer into a single profile. Of course, if an attribute is present in
a single profile, and absent from the others, this is the value we keep in the
consolidated profile. But if an attribute appears with different values across
different profiles, we choose the value coming from the profile corresponding
to the \emph{most recent data release}.

\subsubsection{Unit assignment history} TODO.



